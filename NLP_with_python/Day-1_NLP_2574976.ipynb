{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d2b425c",
   "metadata": {},
   "source": [
    "# 1. What is the purpose of text preprocessing in NLP, and why is it essential before analysis?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1be899ae",
   "metadata": {},
   "source": [
    "# Text preprocessing is a crucial step in natural language processing (NLP) that involves cleaning and transforming raw text data into a format that is suitable for analysis.\n",
    "1.Noise Reduction\n",
    "2.Tokenization\n",
    "3.Lowercasing\n",
    "4.Stopword Removal\n",
    "5.Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdabb71c",
   "metadata": {},
   "source": [
    "# 2. Describe tokenization in NLP and explain its significance in text processing."
   ]
  },
  {
   "cell_type": "raw",
   "id": "88906bfa",
   "metadata": {},
   "source": [
    "Tokenization involves dividing text into smaller components such as words, phrases, or characters. This process is essential for various natural language processing tasks, as it provides organized input for analysis. Tokenization plays a crucial role in tasks like sentiment analysis, language translation, and information retrieval, enabling better language understanding, feature extraction, and model comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eca0fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " Tokenization involves dividing text into smaller components such as words, phrases, or characters. This process is essential for various natural language processing tasks, as it provides organized input for analysis. Tokenization plays a crucial role in tasks like sentiment analysis, language translation, and information retrieval, enabling better language understanding, feature extraction, and model comprehension.\n",
      "\n",
      "Aftr sentence Tokanization:\n",
      " ['Tokenization involves dividing text into smaller components such as words, phrases, or characters.', 'This process is essential for various natural language processing tasks, as it provides organized input for analysis.', 'Tokenization plays a crucial role in tasks like sentiment analysis, language translation, and information retrieval, enabling better language understanding, feature extraction, and model comprehension.']\n",
      "\n",
      "no of sentnces:\n",
      " 3\n",
      "======================================================================\n",
      "\n",
      "Original text:\n",
      " Tokenization involves dividing text into smaller components such as words, phrases, or characters. This process is essential for various natural language processing tasks, as it provides organized input for analysis. Tokenization plays a crucial role in tasks like sentiment analysis, language translation, and information retrieval, enabling better language understanding, feature extraction, and model comprehension.\n",
      "\n",
      "Aftr word Tokanization:\n",
      " ['Tokenization', 'involves', 'dividing', 'text', 'into', 'smaller', 'components', 'such', 'as', 'words', ',', 'phrases', ',', 'or', 'characters', '.', 'This', 'process', 'is', 'essential', 'for', 'various', 'natural', 'language', 'processing', 'tasks', ',', 'as', 'it', 'provides', 'organized', 'input', 'for', 'analysis', '.', 'Tokenization', 'plays', 'a', 'crucial', 'role', 'in', 'tasks', 'like', 'sentiment', 'analysis', ',', 'language', 'translation', ',', 'and', 'information', 'retrieval', ',', 'enabling', 'better', 'language', 'understanding', ',', 'feature', 'extraction', ',', 'and', 'model', 'comprehension', '.']\n",
      "\n",
      "no of words:\n",
      " 65\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text=\"\"\"Tokenization involves dividing text into smaller components such as words, phrases, or characters. This process is essential for various natural language processing tasks, as it provides organized input for analysis. Tokenization plays a crucial role in tasks like sentiment analysis, language translation, and information retrieval, enabling better language understanding, feature extraction, and model comprehension.\"\"\"\n",
    "print('Original text:\\n',text)\n",
    "print()\n",
    "tokenised_sent=sent_tokenize(text)\n",
    "print('Aftr sentence Tokanization:\\n',tokenised_sent)\n",
    "print()\n",
    "print('no of sentnces:\\n',len(tokenised_sent))\n",
    "print('='*70)\n",
    "print()\n",
    "\n",
    "print('Original text:\\n',text)\n",
    "print()\n",
    "tokenised_word=word_tokenize(text)\n",
    "print('Aftr word Tokanization:\\n',tokenised_word)\n",
    "print()\n",
    "print('no of words:\\n',len(tokenised_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038c03f",
   "metadata": {},
   "source": [
    "# 3. What are the differences between stemming and lemmatization in NLP? When would you choose one over the other?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "567bebc5",
   "metadata": {},
   "source": [
    "Stemming involves shortening words to their root form by removing suffixes, enabling quicker processing but potentially yielding non-dictionary words.\n",
    "\n",
    "Lemmatization, on the other hand, derives the base or dictionary form of words through a deeper analysis of vocabulary. While it ensures valid words, it often comes at a higher computational cost compared to stemming.\n",
    "\n",
    "Stemming prioritizes speed in information retrieval, sacrificing precision as it may generate non-words. In contrast, lemmatization, though slower, is preferred when accuracy in language understanding is essential, as it considers the context and meaning of words to provide valid forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cea15c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jagad\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f33f4c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Tokenized words - without stemming:\n",
      "\n",
      "\t ['Tokenization', 'involves', 'dividing', 'text', 'into', 'smaller', 'components', 'such', 'as', 'words', ',', 'phrases', ',', 'or', 'characters', '.', 'This', 'process', 'is', 'essential', 'for', 'various', 'natural', 'language', 'processing', 'tasks', ',', 'as', 'it', 'provides', 'organized', 'input', 'for', 'analysis', '.', 'Tokenization', 'plays', 'a', 'crucial', 'role', 'in', 'tasks', 'like', 'sentiment', 'analysis', ',', 'language', 'translation', ',', 'and', 'information', 'retrieval', ',', 'enabling', 'better', 'language', 'understanding', ',', 'feature', 'extraction', ',', 'and', 'model', 'comprehension', '.']\n",
      "======================================================================\n",
      "\n",
      "Tokenized words - afer stemming are:\n",
      "\t ['token', 'involv', 'divid', 'text', 'into', 'smaller', 'compon', 'such', 'as', 'word', ',', 'phrase', ',', 'or', 'charact', '.', 'thi', 'process', 'is', 'essenti', 'for', 'variou', 'natur', 'languag', 'process', 'task', ',', 'as', 'it', 'provid', 'organ', 'input', 'for', 'analysi', '.', 'token', 'play', 'a', 'crucial', 'role', 'in', 'task', 'like', 'sentiment', 'analysi', ',', 'languag', 'translat', ',', 'and', 'inform', 'retriev', ',', 'enabl', 'better', 'languag', 'understand', ',', 'featur', 'extract', ',', 'and', 'model', 'comprehens', '.']\n",
      "======================================================================\n",
      "lemmarized words:\n",
      " ['Tokenization', 'involve', 'divide', 'text', 'into', 'smaller', 'components', 'such', 'as', 'word', ',', 'phrase', ',', 'or', 'character', '.', 'This', 'process', 'be', 'essential', 'for', 'various', 'natural', 'language', 'process', 'task', ',', 'as', 'it', 'provide', 'organize', 'input', 'for', 'analysis', '.', 'Tokenization', 'play', 'a', 'crucial', 'role', 'in', 'task', 'like', 'sentiment', 'analysis', ',', 'language', 'translation', ',', 'and', 'information', 'retrieval', ',', 'enable', 'better', 'language', 'understand', ',', 'feature', 'extraction', ',', 'and', 'model', 'comprehension', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "\n",
    "stemmed_words=[]\n",
    "\n",
    "for w in tokenised_word:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "    \n",
    "\n",
    "print('='*70)\n",
    "print('Tokenized words - without stemming:\\n\\n\\t',tokenised_word)\n",
    "print('='*70)\n",
    "print('\\nTokenized words - afer stemming are:\\n\\t',stemmed_words)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Initilize the wordnet lemmatizer\n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "#apply lemmatazion to each word\n",
    "lemma_words=[lemma.lemmatize(word,pos='v') for word in tokenised_word ]\n",
    "\n",
    "print('='*70)\n",
    "print('lemmarized words:\\n',lemma_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5858ad35",
   "metadata": {},
   "source": [
    "# 4. Explain the concept of stop words and their role in text preprocessing. How do they impact NLP tasks?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1bc9bbbc",
   "metadata": {},
   "source": [
    "Stop words, such as \"the\" and \"and,\" are frequently eliminated during text preprocessing because they contribute minimal semantic value. This removal enhances the efficiency of NLP tasks by concentrating on more meaningful content.\n",
    "\n",
    "By discarding stop words, the analysis becomes more streamlined, lessening the computational burden for tasks like sentiment analysis and information retrieval. This process amplifies the importance of crucial words in these activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55764da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of words:\t 65\n",
      "======================================================================\n",
      "Tokenized words - with stop words:\n",
      "\n",
      "\t ['Tokenization', 'involves', 'dividing', 'text', 'into', 'smaller', 'components', 'such', 'as', 'words', ',', 'phrases', ',', 'or', 'characters', '.', 'This', 'process', 'is', 'essential', 'for', 'various', 'natural', 'language', 'processing', 'tasks', ',', 'as', 'it', 'provides', 'organized', 'input', 'for', 'analysis', '.', 'Tokenization', 'plays', 'a', 'crucial', 'role', 'in', 'tasks', 'like', 'sentiment', 'analysis', ',', 'language', 'translation', ',', 'and', 'information', 'retrieval', ',', 'enabling', 'better', 'language', 'understanding', ',', 'feature', 'extraction', ',', 'and', 'model', 'comprehension', '.']\n",
      "======================================================================\n",
      "Length after the remoal of stopwords:\t 52\n",
      "======================================================================\n",
      "\n",
      "Tokenized words - afer removing the stopwords are:\n",
      "\t ['Tokenization', 'involves', 'dividing', 'text', 'smaller', 'components', 'words', ',', 'phrases', ',', 'characters', '.', 'This', 'process', 'essential', 'various', 'natural', 'language', 'processing', 'tasks', ',', 'provides', 'organized', 'input', 'analysis', '.', 'Tokenization', 'plays', 'crucial', 'role', 'tasks', 'like', 'sentiment', 'analysis', ',', 'language', 'translation', ',', 'information', 'retrieval', ',', 'enabling', 'better', 'language', 'understanding', ',', 'feature', 'extraction', ',', 'model', 'comprehension', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens=[]\n",
    "for w in tokenised_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_tokens.append(w)\n",
    "print('Length of words:\\t',len(tokenised_word))\n",
    "print('='*70)\n",
    "print('Tokenized words - with stop words:\\n\\n\\t',tokenised_word)\n",
    "print('='*70)\n",
    "print('Length after the remoal of stopwords:\\t',len(filtered_tokens))\n",
    "print('='*70)\n",
    "print('\\nTokenized words - afer removing the stopwords are:\\n\\t',filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa5f5a",
   "metadata": {},
   "source": [
    "# 5. How does the process of removing punctuation contribute to text preprocessing in NLP? What are its benefits?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "003666df",
   "metadata": {},
   "source": [
    "The elimination of punctuation supports text normalization by removing unnecessary characters, thereby improving readability and consistency for analysis. This process prevents interference with the understanding of language models and enhances tokenization accuracy by establishing clear word boundaries for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d226dbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " Tokenization involves dividing text into smaller components such as words, phrases, or characters. This process is essential for various natural language processing tasks, as it provides organized input for analysis. Tokenization plays a crucial role in tasks like sentiment analysis, language translation, and information retrieval, enabling better language understanding, feature extraction, and model comprehension.\n",
      "======================================================================\n",
      "after removing puntuations:\n",
      "\n",
      "['Tokenization', 'involves', 'dividing', 'text', 'into', 'smaller', 'components', 'such', 'as', 'words', 'phrases', 'or', 'characters', 'This', 'process', 'is', 'essential', 'for', 'various', 'natural', 'language', 'processing', 'tasks', 'as', 'it', 'provides', 'organized', 'input', 'for', 'analysis', 'Tokenization', 'plays', 'a', 'crucial', 'role', 'in', 'tasks', 'like', 'sentiment', 'analysis', 'language', 'translation', 'and', 'information', 'retrieval', 'enabling', 'better', 'language', 'understanding', 'feature', 'extraction', 'and', 'model', 'comprehension']\n"
     ]
    }
   ],
   "source": [
    "words=[word for word in tokenised_word if word.isalpha()]\n",
    "\n",
    "print('Original text:\\n',text)\n",
    "print('='*70)\n",
    "\n",
    "print('after removing puntuations:\\n')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51b35f",
   "metadata": {},
   "source": [
    "# 6. Discuss the importance of lowercase conversion in text preprocessing. Why is it a common step in NLP tasks?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d058e66",
   "metadata": {},
   "source": [
    "Converting text to lowercase standardizes word representation, promoting uniformity and aiding in matching. This step reduces vocabulary size, enhancing model generalization and accuracy in tasks like text analysis and feature extraction. It is a common practice in NLP to ensure consistency in text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2da2008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " Tokenization involves dividing text into smaller components such as words, phrases, or characters. This process is essential for various natural language processing tasks, as it provides organized input for analysis. Tokenization plays a crucial role in tasks like sentiment analysis, language translation, and information retrieval, enabling better language understanding, feature extraction, and model comprehension.\n",
      "======================================================================\n",
      "after lowering the case:\n",
      "\n",
      "['tokenization', 'involves', 'dividing', 'text', 'into', 'smaller', 'components', 'such', 'as', 'words', ',', 'phrases', ',', 'or', 'characters', '.', 'this', 'process', 'is', 'essential', 'for', 'various', 'natural', 'language', 'processing', 'tasks', ',', 'as', 'it', 'provides', 'organized', 'input', 'for', 'analysis', '.', 'tokenization', 'plays', 'a', 'crucial', 'role', 'in', 'tasks', 'like', 'sentiment', 'analysis', ',', 'language', 'translation', ',', 'and', 'information', 'retrieval', ',', 'enabling', 'better', 'language', 'understanding', ',', 'feature', 'extraction', ',', 'and', 'model', 'comprehension', '.']\n"
     ]
    }
   ],
   "source": [
    "lower_words=[word.lower() for word in tokenised_word]\n",
    "print('Original text:\\n',text)\n",
    "print('='*70)\n",
    "\n",
    "print('after lowering the case:\\n')\n",
    "print(lower_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce05c94f",
   "metadata": {},
   "source": [
    "# 7. Explain the term \"vectorization\" concerning text data. How does techniques like CountVectorizer contribute to text preprocessing in NLP?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "021cf240",
   "metadata": {},
   "source": [
    "Vectorization is the process of transforming text into numerical vectors, where words or phrases are represented by numerical values. This enables machine learning algorithms to effectively process and analyze textual data. Methods such as CountVectorizer translate text into a matrix of word frequencies, facilitating feature extraction by quantifying occurrences of words. This approach allows models to interpret text data for tasks like classification or clustering in NLP, ultimately enhancing model performance and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1982646d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:\n",
      " ['analysis' 'and' 'as' 'better' 'characters' 'components' 'comprehension'\n",
      " 'crucial' 'dividing' 'enabling' 'essential' 'extraction' 'feature' 'for'\n",
      " 'in' 'information' 'input' 'into' 'involves' 'is' 'it' 'language' 'like'\n",
      " 'model' 'natural' 'or' 'organized' 'phrases' 'plays' 'process'\n",
      " 'processing' 'provides' 'retrieval' 'role' 'sentiment' 'smaller' 'such'\n",
      " 'tasks' 'text' 'this' 'tokenization' 'translation' 'understanding'\n",
      " 'various' 'words']\n",
      "**********************************************************************\n",
      "Token counts matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vector=CountVectorizer()\n",
    "\n",
    "x=vector.fit_transform(tokenised_word)\n",
    "\n",
    "feature_names=vector.get_feature_names_out()\n",
    "\n",
    "print('Feature names:\\n',feature_names)\n",
    "print('*'*70)\n",
    "print('Token counts matrix:')\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c5d5e",
   "metadata": {},
   "source": [
    "# 8. Describe the concept of normalization in NLP. Provide examples of normalization techniques used in text preprocessing."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7f53e37",
   "metadata": {},
   "source": [
    "Normalization in NLP seeks to convert diverse word forms into a standardized format, improving analysis by minimizing redundancy. Methods like stemming (shortening words to their root form, such as changing \"running\" to \"run\") and lemmatization (reducing words to their base or dictionary form, like transforming \"better\" to \"good\") play a role in standardizing text. Additionally, actions like Punctuation Removal and Stop Word Removal contribute to maintaining consistency in language representation. These practices enhance model comprehension and accuracy, particularly in tasks like sentiment analysis or information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcb403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
